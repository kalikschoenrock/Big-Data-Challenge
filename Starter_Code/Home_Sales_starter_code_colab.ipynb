{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_KW73O2e3dw",
        "outputId": "57668357-2d0e-44ae-f494-c178d4b9ed6f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'apt-get' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n",
            "The system cannot find the path specified.\n",
            "'wget' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n",
            "tar: Error opening archive: Failed to open '$SPARK_VERSION-bin-$HADOOP_VERSION.tgz'\n"
          ]
        },
        {
          "ename": "Exception",
          "evalue": "Unable to find py4j in /content/spark-3.3.1-bin-hadoop2\\python, your SPARK_HOME may not be configured correctly",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\kalik\\anaconda3\\lib\\site-packages\\findspark.py:159\u001b[0m, in \u001b[0;36minit\u001b[1;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 159\u001b[0m     py4j \u001b[39m=\u001b[39m glob(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(spark_python, \u001b[39m\"\u001b[39;49m\u001b[39mlib\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mpy4j-*.zip\u001b[39;49m\u001b[39m\"\u001b[39;49m))[\u001b[39m0\u001b[39;49m]\n\u001b[0;32m    160\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mIndexError\u001b[39;00m:\n",
            "\u001b[1;31mIndexError\u001b[0m: list index out of range",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\kalik\\Desktop\\data_course\\Big-Data-Challenge\\Starter_Code\\Home_Sales_starter_code_colab.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kalik/Desktop/data_course/Big-Data-Challenge/Starter_Code/Home_Sales_starter_code_colab.ipynb#W0sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# Start a SparkSession\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kalik/Desktop/data_course/Big-Data-Challenge/Starter_Code/Home_Sales_starter_code_colab.ipynb#W0sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mfindspark\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/kalik/Desktop/data_course/Big-Data-Challenge/Starter_Code/Home_Sales_starter_code_colab.ipynb#W0sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m findspark\u001b[39m.\u001b[39;49minit()\n",
            "File \u001b[1;32mc:\\Users\\kalik\\anaconda3\\lib\\site-packages\\findspark.py:161\u001b[0m, in \u001b[0;36minit\u001b[1;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[0;32m    159\u001b[0m         py4j \u001b[39m=\u001b[39m glob(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(spark_python, \u001b[39m\"\u001b[39m\u001b[39mlib\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mpy4j-*.zip\u001b[39m\u001b[39m\"\u001b[39m))[\u001b[39m0\u001b[39m]\n\u001b[0;32m    160\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mIndexError\u001b[39;00m:\n\u001b[1;32m--> 161\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\n\u001b[0;32m    162\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUnable to find py4j in \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, your SPARK_HOME may not be configured correctly\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    163\u001b[0m                 spark_python\n\u001b[0;32m    164\u001b[0m             )\n\u001b[0;32m    165\u001b[0m         )\n\u001b[0;32m    166\u001b[0m     sys\u001b[39m.\u001b[39mpath[:\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m sys_path \u001b[39m=\u001b[39m [spark_python, py4j]\n\u001b[0;32m    167\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    168\u001b[0m     \u001b[39m# already imported, no need to patch sys.path\u001b[39;00m\n",
            "\u001b[1;31mException\u001b[0m: Unable to find py4j in /content/spark-3.3.1-bin-hadoop2\\python, your SPARK_HOME may not be configured correctly"
          ]
        }
      ],
      "source": [
        "# Activate Spark in our Colab notebook.\n",
        "import os\n",
        "# Find the latest version of spark 3.0  from http://www.apache.org/dist/spark/ and enter as the spark version\n",
        "spark_version = \"spark-3.3.1\"\n",
        "# spark_version = 'spark-3.<enter version>'\n",
        "os.environ[\"SPARK_VERSION\"]=spark_version\n",
        "# Verify the name of the hadoop2 TGZ file into the spark version folder, eg: spark-3.3.1-bin-hadoop2.tgz\n",
        "hadoop_version = \"hadoop2\"\n",
        "os.environ[\"HADOOP_VERSION\"] = \"hadoop2\"\n",
        "\n",
        "# Install Spark and Java\n",
        "!apt-get update\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-$HADOOP_VERSION.tgz\n",
        "!tar xf $SPARK_VERSION-bin-$HADOOP_VERSION.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# Set Environment Variables\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-{hadoop_version}\"\n",
        "\n",
        "# Start a SparkSession\n",
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2XbWNf1Te5fM"
      },
      "outputs": [],
      "source": [
        "# Import packages\n",
        "from pyspark.sql import SparkSession\n",
        "import time\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"SparkSQL\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wOJqxG_RPSwp"
      },
      "outputs": [],
      "source": [
        "# 1. Read in the AWS S3 bucket into a DataFrame.\n",
        "from pyspark import SparkFiles\n",
        "url = \"https://2u-data-curriculum-team.s3.amazonaws.com/dataviz-classroom/v1.1.2/22-big-data/home_sales_revised.csv\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RoljcJ7WPpnm"
      },
      "outputs": [],
      "source": [
        "# 2. Create a temporary view of the DataFrame.\n",
        "\n",
        "df.createOrReplaceTempView('sales')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "L6fkwOeOmqvq"
      },
      "outputs": [],
      "source": [
        "# 3. What is the average price for a four bedroom house sold in each year rounded to two decimal places?\n",
        "spark.sql(\"SELECT year, round(avg(price),2) as `Average 4-bedroom Sale Price` FROM sales WHERE bedrooms=4 GROUP BY year\").show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "l8p_tUS8h8it"
      },
      "outputs": [],
      "source": [
        "# 4. What is the average price of a home for each year the home was built that have 3 bedrooms and 3 bathrooms rounded to two decimal places?\n",
        "spark.sql(\"SELECT date_built as Year, round(avg(price),2) as `Average 3-bed, 3-bath Sale Price` FROM sales WHERE bedrooms=3 and bathrooms=3 GROUP BY date_built\").show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Y-Eytz64liDU"
      },
      "outputs": [],
      "source": [
        "# 5. What is the average price of a home for each year built that have 3 bedrooms, 3 bathrooms, with two floors,\n",
        "# and are greater than or equal to 2,000 square feet rounded to two decimal places?\n",
        "\n",
        "spark.sql(\"SELECT date_built as Year, round(avg(price),2) as `Average Sale Price` FROM sales WHERE bedrooms=3 and bathrooms=3 and floors=2 and sqft_living>2000 GROUP BY date_built\").show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUrfgOX1pCRd",
        "outputId": "0a645986-7179-440c-c0bc-345f1693f8af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- 7.200241088867188e-05 seconds ---\n"
          ]
        }
      ],
      "source": [
        "# 6. What is the \"view\" rating for the average price of a home, rounded to two decimal places, where the homes are greater than\n",
        "# or equal to $350,000? Although this is a small dataset, determine the run time for this query.\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "spark.sql(\"SELECT round(avg(view),2) as `Average View Rating`, round(avg(price),2) as `Average Sale Price` FROM sales WHERE price>=350000 GROUP BY date_built\").show()\n",
        "\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "KAhk3ZD2tFy8"
      },
      "outputs": [],
      "source": [
        "# 7. Cache the the temporary table home_sales.\n",
        "spark.sql(\"cache table sales\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4opVhbvxtL-i"
      },
      "outputs": [],
      "source": [
        "# 8. Check if the table is cached.\n",
        "spark.catalog.isCached('home_sales')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GnL46lwTSEk",
        "outputId": "63c7dc50-d96a-4a48-97b6-91a446cdb973"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- 5.459785461425781e-05 seconds ---\n"
          ]
        }
      ],
      "source": [
        "# 9. Using the cached data, run the query that filters out the view ratings with average price of greater than $350,000. \n",
        "# Determine the runtime and compare it to uncached runtime.\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "\n",
        "\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Qm12WN9isHBR"
      },
      "outputs": [],
      "source": [
        "# 10. Partition the home sales dataset by the date_built field.\n",
        "df.write.partitionBy(\"date_built\").mode(\"overwrite\").parquet(\"sales_partitioned\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "AZ7BgY61sRqY"
      },
      "outputs": [],
      "source": [
        "# 11. Read the formatted parquet data.\n",
        "p_df=spark.read.parquet('sales_partitioned')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "J6MJkHfvVcvh"
      },
      "outputs": [],
      "source": [
        "# 12. Create a temporary table for the parquet data.\n",
        "p_df.createOrReplaceTempView('sales_partitioned')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_Vhb52rU1Sn",
        "outputId": "d6748ea6-d70a-41fd-dcb8-c214a85e949e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- 7.104873657226562e-05 seconds ---\n"
          ]
        }
      ],
      "source": [
        "# 13. Run the query that filters out the view ratings with average price of greater than $350,000 with the parquet DataFrame. \n",
        "# Round your average to two decimal places. \n",
        "# Determine the runtime and compare it to the cached version. \n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "\n",
        "\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "hjjYzQGjtbq8"
      },
      "outputs": [],
      "source": [
        "# 14. Uncache the home_sales temporary table.\n",
        "spark.sql(\"uncache table sales\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Sy9NBvO7tlmm"
      },
      "outputs": [],
      "source": [
        "# 15. Check if the home_sales is no longer cached\n",
        "\n",
        "if spark.catalog.isCached(\"sales\"):\n",
        "  print(\"a table is till cached\")\n",
        "else:\n",
        "  print(\"all clear\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Si-BNruRUGK3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "nteract": {
      "version": "0.28.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
